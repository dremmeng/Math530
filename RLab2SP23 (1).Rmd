---
title: "DSCI/MATH 530 RLab Two"
author: " "
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
This  lab in R is an introduction to the exploratory analysis of data. Here we will use some actual data sets and also some samples that are generated by Monte Carlo simulations. This second case will show how to examine some probability distributions and principles using Monte Carlo experiments. It is also a stealth presentation of the concepts of *bagging* and *bootstrapping*. 
 
## Submission 
Submitting through gradescope. Complete this assignment using R Markdown and converting to pdf.  Make sure each problem is on a separate page. 
use  \verb+\newpage+  before each problem to insert a page break.

You can also refer to the .Rmd file for this assignment to have an example of this markdown document. Note however, that this version is setup to also use Latex for some math and conversion directly to pdf. 

## Background on R
This assignment will give some specific hints about using the R language. However, you should look at the R tutorial included in the course pages. 
(See **>>Start Here** in the top menu and then look for Getting Ready -- the world of R.) for much more details. In particular you may want to review how for loops are coded in R and the use of subscripts. 

Be careful to use  \verb+ ```{r}+ to begin a code block that will be run.
Note that the Rmarkdown for this assignment  has some formatted code blocks that  are not run and this is achieved by omitting the 
\verb+{r}+  and just using \verb+```+

Note: In R anything  in the workspace is usually referred to as an *object*. Sometmes these are data sets read in or other results from doing some computations. They might also have specific types, such as vectors, matrices or data frames, but we would call them all objects. 

E.g. ```obj<- 1:10 ``` and we would call ```obj``` an *object* in the workspace and more specifically in this case it is a vector. 

\newpage
## 1 Bagging a sample.

The technique of bagging is used in machine learning to generate different representations of a data set and helps to avoid overfitting. It is also the basic technique used in bootstrapping a technique for statistical inference. 

If a sample has  $n$ values  $x_1, x_2, \ldots, x_n$ then a bagged sample is found by randomly selecting these values with replacement to create another sample of size $n$. If ```X``` is the vector of values in R  of size 50 then 
```
bagSample<- sample( X,50 , replace = TRUE)
```
will give a new sample of size 50, drawn randomly from ```X ```  with replacement.

Here is the interesting feature of this technique: on average about 1/3 of
the values in ```X``` will *not* be part of the bagged sample. 
Instead some values of ```X``` will be repeated. One can use a simple probability argument to show the expected fraction should converge to  
\[ 1/e \approx 1/2.7182 = .3679 \]
 as the sample size gets large. However, one can also test this by Monte Carlo. Here is some R code to do it. I am also creating a random sample to use for testing. If we are just intereted in the *number* of values in the out-of-bag sample, the actual sample values do not matter (why?).
You might want to run the code below and also print out ```X``` and the ```bagSample``` just make sure you understand how this works. 

```{r}
set.seed( 123)
n<- 50
X<- runif( n)
bagSample<- sample( X,n , replace = TRUE)
m<- length( unique( bagSample))
fracMissing<-  1- m/n
print( fracMissing)
```
Check the help file for the **unique** function if you are not familiar with this operation. 

Finally, lets generate 2000 bagged samples so we can examine the distribution of the number left out. This uses a **for** loop and saves the values in an array. I like to initialize the array with missing values to start. Note that this example is also a general format for looping  over cases and saving the computation.
```{r}
set.seed( 123)
n<- 50
X<- runif( n)
nBag<- 2000
outOfBagSize<- rep(NA, nBag)

for(  k in 1:nBag){
bagSample<- sample( X,n , replace = TRUE)
m<- length( unique( bagSample))

outOfBagSize[k]<- n-m
}
mean(  outOfBagSize/n)
exp( -1)

```
Hey! Pretty close to  $1/e$ !
Some more statistics (be sure to load the fields package )

```{r}
suppressMessages(library( fields))
stats( outOfBagSize/n )
```

#1(a) 
Make a histogram of the fraction of values not in the bagged sample (called the out-of-bag fraction) and superimpose a normal probability on top of your histogram (see the example for R lab 1 to do this.) For the normal use the sample mean and standard deviation for the parameters. The sample deviation in R can be computed using the function  **sd**.  Comment on whether the normal distribution fits the histogram. 

#1(b)
Let $M$ be the number of values in the out-bag-sample for a given draw. 
A natural guess at the distribution for $M$ is binomial with $p= 1/e$ and $n$ the sample size of the full data. This has  the right expected value for large $n$ but it is not clear that $M$ is really binomially distributed. We also know that the variance of a binonmial is $n (p) (1-p)$ and so the variance of the fraction, $X/n$ is $p(1-p)/n$ and with standard deviation $\sqrt{p(1-p)/n}$. Based on your Monte Carlo results do the out-of-bag fractions have a standard deviation close to this?

\newpage
# 2 Daily weather measurements for Boulder, CO
In this question you will investigate the seasonality of daily rainfall and temperature recorded at Boulder, CO. 

First download the R binary dataset file  **BoulderDaily.rda** and 
move this file to the folder where you are working on the homework. 
R is always pointed at a working directory on your computer. You can find out which directory  this is  by typing ```getwd()``` in the console. You can set the working directory to another folder either by using ```setwd``` or the option in the *Session* menu in the top line of RStudio.
Here I load the data and check the first and last four rows. 

```{r}
load("BoulderDaily.rda")
# and examine first few months of data are missing ...
head(BoulderDaily,4 )
tail( BoulderDaily,4)
```

The column *tmean* is simply the average of the daily min and max.

This is a *data.frame* so columns can have different data types.
Access each column using a subscript or a \$ or its name. 
E.g. ``` BoulderDaily[,5] ``` or ```BoulderDaily$tmin```
```BoulderDaily[,'tmin']``` will be **tmin**, the daily minimum temperature.
 The last column *date* is a date object that works with the lubridate package. Dates can be difficult to work without these kinds of extra tools in R. 
 
# 2(a) 
 Boxplots are most useful for comparing several datasets or subsets of the data. The **boxplot** function allows for a handy formula syntax to create groups of boxplots. 
 Below are boxplots for the mean daily Boulder temperature by month.
 (Read the \verb+~+  as *by* ). 
 
 ```{r}
 boxplot(BoulderDaily$tmean ~ BoulderDaily$month )
 ```
 
 Add a title and axis labels to this plot.  Comment on how the distribution changes over the yearly cycle. Are these distributions symmetric or skewed? Which months are the most variable? Which two months have about the same
 distribution?
 
# 2(b) 
Now create boxplots to examine the distrbution of daily max temperatures (**tmax**) by year. Do you see any evidence for a warming temperature trend? Add a horiztonal line at the overal mean
``` abline( h= mean( BoulderDaily$tmax, na.rm=TRUE), col="magenta") ``` to help discern any subtle trends.


# 2(c)
Use boxplots over month to see if daily precip in Boulder also has a sesonal cycle. The distribution is highly skewed, however, why it this the case? Give some susggestions how to subset or transform the data to get more detail about its distribution. 

\newpage
# 3 Correlation in daily maximum temperatures
Following the textbook the correlation coefficient between two variables is a measure of  the strength of predictability under the assumption of a linear relationship. This problem is about looking at how strong is the
predictability of max temperature based on the previous day's value. 
In order to do this we need to lag the temperature data so that we can match each day with the previous day's value. See the code below. This involves a little bit of indexing and we add the lagged variable to the dataframe

```{r}
N<- nrow(BoulderDaily )
# the first value is missing because it the day before # the data starts
tmaxLag1<- c(NA, BoulderDaily$tmax[1: (N-1)])
BoulderDaily$tmaxLag1<- tmaxLag1
# to check compare the values from days 201 to 205
index<- 201:205
BoulderDaily[index,]

```

# 3(a) 
To find the correlation among these values use 
``` cor( BoulderDaily$tmaxLag1 , BoulderDaily$tmax, use="complete" ) ``` where the *complete* means remove all missing observations from both variables. Interpret this value. 


# 3(b) 
Make a scatterplot with the laged tmax on the x axis and the tmax on the y. Does this look like a linear relationship. 

Alternatively  install and load the fields library try the plot
```
library( fields)
bplot.xy( BoulderDaily$tmaxLag1,BoulderDaily$tmax,
N=10)
```
Here the  X- values have been binned and a boxplot is drawn for each set of binned values. Add a title and labels and adjust the number of bins (**N**) to give the clearest pattern. (Choosing **N** is little like Goldilocks and her oatmeal -- not too small but not too large!)

Based on these results which temperatures can be more accurately predicted. Can your conclusions be made from the scatterplot?

3(c) 
The following code considers each month of data and finds the correlation between the lagged tmax and tmax.
The idea is that we want to see if the strength of the correlation varies over month. Just for reference we compute the standard deviation for each month as well. You can assume that we have checked each month and have roughly a linear relationship between the two variables. So it makes sense to interpret the correlation. 

```{r}
corTable<- rep( NA, 12)
sdTable<- rep( NA, 12)
  for( k in 1:12){
    ind<- which( BoulderDaily$month==k )
      corTable[k]<- cor( BoulderDaily$tmaxLag1[ind] ,
                           BoulderDaily$tmax[ind],
                           use="complete" )
      sdTable[k]<- sd( BoulderDaily$tmax[ind], na.rm=TRUE )
  }

```

Based on these results is there 
substantially more predictability in one month relative
to another? Include a figure to support your answer.

**EXTRA CREDIT** Explain why the correlation for the indivdual months is lower than the correlation for the complete data. 



























